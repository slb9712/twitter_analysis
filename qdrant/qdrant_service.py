import os

from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
import uuid
import numpy as np
from hdbscan import HDBSCAN


class QdrantService:
    _client = None
    COLLECTION_NAME = "news_embedding"

    @classmethod
    def get_client(cls):
        if cls._client is None:
            cls._client = QdrantClient(
                host=os.getenv("QDRANT_HOST"),
                port=os.getenv("QDRANT_PORT"),
                timeout=60,
                api_key='123456',
                https=False,
            )
            cls._init_collection()
        return cls._client

    @classmethod
    def _init_collection(cls):
        client = cls.get_client()
        collections = client.get_collections().collections
        collection_names = [c.name for c in collections]

        if cls.COLLECTION_NAME not in collection_names:
            client.create_collection(
                collection_name=cls.COLLECTION_NAME,
                vectors_config=VectorParams(
                    size=768,
                    distance=Distance.DOT
                )
            )

    def insert_embeddings(self, embeddings, metadata_list=None):
        """æ’å…¥åµŒå…¥å‘é‡åˆ°Qdrant"""
        client = self.get_client()
        points = []
        for idx, vector in enumerate( embeddings):
            # è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€ID
            point_id = str(uuid.uuid4())

            payload = {
            }
            if metadata_list and idx < len(metadata_list):
                payload.update(metadata_list[idx])

            points.append(PointStruct(
                id=point_id,
                vector=vector,
                payload=payload
            ))

        return client.upsert(
            collection_name=self.COLLECTION_NAME,
            wait=True,
            points=points
        )

    def run_hdbscan_clustering(self, vectors, payloads, min_cluster_size=3, project_name=None):
        clusterer = HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean')
        labels = clusterer.fit_predict(vectors)
        clustered = {}
        for label, payload in zip(labels, payloads):
            clustered.setdefault(label, []).append(payload)
        
        return clustered

        # print(f"\n=== Project: {project_name} ===")
        # for cluster_id, items in clustered.items():
        #     print(f"\nğŸ§  Cluster {cluster_id} ({len(items)} items):")
        #     for item in items[:5]:  # æ¯ç°‡æœ€å¤šå±•ç¤ºå‰5ä¸ª
        #         print(f"  - {item.get('source_name')} | id: {item.get('source_id')} | db: {item.get('source_db')}")

        # print(f"\nâœ… æ€»å…±å‘ç° {len(set(labels)) - (1 if -1 in labels else 0)} ä¸ªç°‡ï¼Œ{np.sum(labels == -1)} æ¡ä¸ºå™ªå£°ï¼ˆcluster -1ï¼‰")


    def fetch_vectors_by_payloads(self, search_conditions, collection_name=None):
        """
        æ ¹æ®payloadä¸­çš„source_nameå’Œsource_idæ‰¹é‡æ£€ç´¢å‘é‡ã€‚
        search_conditions: List[Dict], ä¾‹å¦‚ [{"source_name": ..., "source_id": ...}, ...]
        è¿”å›: vectors(np.ndarray), payloads(list)
        """
        client = self.get_client()
        collection = collection_name or self.COLLECTION_NAME
        all_vectors = []
        all_payloads = []
        for cond in search_conditions:
            # æ„é€ payloadè¿‡æ»¤æ¡ä»¶
            query = {
                "must": [
                    {"key": "source_name", "match": {"value": cond["source_name"]}},
                    {"key": "source_id", "match": {"value": cond["source_id"]}}
                ]
            }
            result = client.scroll(
                collection_name=collection,
                limit=10,
                with_payload=True,
                with_vectors=True,
                scroll_filter=query
            )[0]
            for point in result:
                all_vectors.append(point.vector)
                all_payloads.append(point.payload)
        if all_vectors:
            return np.array(all_vectors), all_payloads
        else:
            return [], []